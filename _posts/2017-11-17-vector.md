---
layout: post
title: High Quality Text를 위한 알고리즘
date: 2017-11-17
description: 넘쳐나는 원고들 속에서 주옥 같은 작품들을 인간 편집자가 족집게처럼 거르기란 여간 쉬운 일이 아닐 것이다. 하지만 인공지능이 미리 선별해 놓은 소수의 후보 원고들 중에서만 인간 편집자가 선택하게 된다면? 이 같은 방식은 인공지능과 인간지능의 불완전성을 상호 보완하는 대안 가운데 하나가 될 것이라 믿고 있다.
img: "https://i.imgur.com/yIAmVcZ.jpg" # Add image post (optional)
tags: [Quality, Algorithm, Vectorization]
---

이번 글에서는 Quality Project의 기술적 지향점을 살펴보고 이 지향점의 핵심이 되는 단어, 문서 벡터화(Vectorization) 기술을 살펴보도록 하겠다. 



<br>

## Quality Project에서의 기술의 역할

[앞선 글](https://brunch.co.kr/@gorats/1)에서 우리는 어떤 글이 질 좋은 글인지 고민하는 과정을 소개했다. 여전히 완전한 답을 내리기 어려운 부분이 많지만, 현재로서는 도전적이고 창의적인 일에 도전하려는 사람들에게 **영감(insight)과 지식(knowledge)을 주는 글**이라는 잠정 결론에 도달했다. Quality Project에서 기술이 맡아야 할 첫번째 역할은, [앞선 글](https://brunch.co.kr/@gorats/1)에서 언급했던 것처럼 인간 편집자의 정확한 큐레이션을 돕는 데 초점이 맞춰져야 한다.

Quality Project는 네트워크 기반의 기술 플랫폼을 지향하고 있는 만큼, 개별 사용자들의 관심이나 취향에 맞게 적절한 컨텐츠를 **추천(recommendation)**하는 역할도 당연히 수행해야 한다. 이는 심지어 오프라인 서점들도 고민하고 있는 지점이기도 하다. 이와 관련 일본 서점체인 '츠타야(TSUTAYA)'는 호텔에나 있을 법한 컨시어지(Concierge) 담당 직원을 두고, 고객들에게 책을 추천해주고 있다.

Quality Project에서 기술은 인간 편집자의 큐레이션, 인간 독자의 컨텐츠 소비를 돕는 역할을 수행해야 한다. 이 과정에서 사람의 지적 활동이 인공지능에 의해 지배 받는다는 느낌을 주지 않을 수록 최선이 될 것이라고 믿는다. 글을 읽고 이를 바탕으로 도전, 창의, 혁신을 일궈내는 것 자체가 인간성(人間性)의 핵심이라 생각하기 때문이다. 

다시 말해 물과 공기처럼 평소엔 그 존재감을 알아차리기 어렵지만, 없으면 안되거나 대단히 불편한 '그 무엇'이 되어야 한다는 이야기다. 그래서 Quality Project는 기술(기반) 프로젝트이지만 기술(중심) 프로젝트는 아니다.



<br>

## 벡터화 기술

Quality Project 기술의 핵심 요소는 자연언어처리(Natural Language Processing), 딥러닝(Deep Learning), 기계학습(Machine Learning) 등이다. 그 가운데 가장 기본이 되는 것은 문서, 문장, 구(phrase), 단어를 벡터로 바꾸는 벡터화 기술이다. Quality Project가 다루는 데이터는 기본적으로 텍스트 기반 말뭉치(corpus)이고, 컴퓨터는 숫자밖에 이해하지 못하기 때문에 벡터화는 Quality Project 기술의 첫 단추가 될 수밖에 없다.

물리학에서 벡터(vector)는 크기와 방향을 동시에 가지는 물리적 양을 가리킨다. 선형대수학(Linear Algebra)에서는 벡터를 어떤 공간상에 존재하는 하나의 포인트로 이해하는데, 원점(0)을 고려하면 물리학에서 정의하는 벡터와 본질적으로 다르지는 않다. 벡터는 사칙연산뿐 아니라 미분(differential)도 가능하다. 컴퓨터가 계산(=이해)할 수 있다는 얘기다. 예컨대 단어를 2차원 벡터로 바꾸면 다음과 같은 일이 가능해진다.

 

$$King - Queen = Kings - Queens$$

<a href="https://imgur.com/agTBWiT"><img src="https://i.imgur.com/agTBWiT.png" width="400px" title="source: imgur.com" /></a>



벡터화를 하면 유사도(similarity) 또한 계산할 수 있게 된다. 고등학교 수학 시간에 배운 코사인 법칙을 활용해 유도하면 두 벡터의 내적(inner product)은 두 벡터 사이의 코사인(cosine)이 된다. 이 값이 클 수록 두 벡터가 유사하다고 결론을 내는 것이다. 예컨대 아래 그림처럼 벡터화가 되어 있을 경우 'apple'은 'boat'보다는 'banana'와 비슷하다.



<a href="https://imgur.com/clZriiL"><img src="https://i.imgur.com/clZriiL.png" width="300px" title="source: imgur.com" /></a>



<br>

##One-Hot-Encoding

그러면 문서, 문장, 구, 단어를 어떻게 벡터화해야 효과적일까. 이 문제는 지난 수십년 간 연구되어온 주제이고, 정답이 있을 수는 없다. 지금까지 제안된 단어 벡터화 기술 몇 가지를 소개해 보려고 한다.

가장 간단한 방식은 말뭉치에 존재하는 전체 단어수($V$)만큼의 차원을 갖는 영벡터를 만들고 해당 단어 인덱스만 1로 바꿔주는 것이다. 예컨대 내가 가지고 있는 말뭉치에 단어가 '개', '고양이', '사자' 세 개만 있다고 치자. 그러면 개는 $[1,0,0]$, 고양이와 사자는 각각 $[0,1,0]$, $[0,0,1]$ 이렇게 벡터화하는 방식이다. 이 방식을 One-Hot-Encoding이라고 한다.  

하지만 한 언어에 단어가 보통 10만개 이상임을 고려할 때 이 방식은 보통 비효율적인 것이 아니다. 뿐만 아니라 두 벡터의 내적이 무조건 0이 되어 벡터 유사도를 계산할 수 없다.  



<br>

## 빈도 기반 방법

빈도(count) 기반 방법은 말 그대로 빈도 수를 세어 벡터화하는 기법이다. 이 기법에도 다양한 변종이 있으나 가장 간단한 방식은 문서 단어 행렬(Document Term Matrix)을 만드는 것이다. 예컨대 다음과 같은 3개 문서를 아래와 같은 행렬로 바꾼다.

- **문서1** : 나,는,학교,에,가,ㄴ,다
- **문서2** : 학교,에,가,는,영희
- **문서3** : 나,는,영희,는,좋,다

|  -   | 문서1  | 문서2  | 문서3  |
| :--: | :--: | :--: | :--: |
|  나   |  1   |  0   |  0   |
|  는   |  1   |  1   |  2   |
|  학교  |  1   |  1   |  0   |
|  에   |  1   |  1   |  0   |
|  가   |  1   |  1   |  0   |
|  ㄴ   |  1   |  0   |  0   |
|  다   |  1   |  0   |  1   |
|  영희  |  0   |  1   |  1   |
|  좋   |  0   |  0   |  1   |

위 행렬에서 행벡터(row vector)가 그대로 단어 벡터가 되며, 열벡터(column vector)가 문서 벡터가 된다. 하지만 실제로는 단어, 문서의 수가 엄청나게 많다. 예컨대 분석 대상 말뭉치의 단어수가 10만 개라면 행벡터의 차원수는 10만이나 되어 버리는 것이다. 

이 문제 해결을 위해 다양한 차원축소(dimensionality reduction) 기법이 제시되었다. 이 가운데 가장 유명한 방식이 [잠재의미분석(Latent Semantic Analysis)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/)이며, 최신 기법이 미국 스탠포드대학에서 개발한 [GloVe](https://nlp.stanford.edu/projects/glove/)이다. 이 방식은 구글, 네이버 등 검색엔진에서 문서를 벡터화할 때 널리 쓰인다고 한다.



<br>

## 예측 기반 방법

국어학자들에 따르면 한국어 품사 분류에 가장 결정적인 기준은 바로 기능(function)이다. 여기서 말하는 기능은 한 단어가 문장 가운데서 다른 단어와 맺는 문법적 관계를 가리킨다. 그런데 한국어에서는 단어의 기능이 분포(distribution)와 매우 밀접한 관련을 맺고 있다. 분포란 단어의 등장 순서나 위치를 말한다. 

다시 말해 기능은 단어가 문장 내에서 어떤 역할을 하는지, 분포는 그 단어가 어느 자리에 있는지를 나타낸다. 비유하자면 최순실 씨가 박근혜정부의 실세(기능)였던 건 박 전 대통령과 자주 만나고 가까이에 있었기(분포) 때문이다. 이처럼 기능과 분포는 개념적으로 엄밀히 다르지만, 둘 사이에는 밀접한 관련을 지닌다.

구글이 2013년 공개한 [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) 기술은 이러한 아이디어에 착안한 것이다. 이웃한 단어들끼리는 그 의미 또한 비슷할 것이라는 가정 하에 중심에 있는 단어로 이웃 단어를 예측(predict)하는 간단한 신경망(Neural Network) 모델을 만들었다. 이 신경망 모델 학습으로 도출된 가중치 행렬이 최종 단어벡터가 된다. Word2Vec 기법(Skip-Gram)이 단어를 예측하는 과정은 다음 그림과 같다.



<a href="https://imgur.com/8zNRwsn"><img src="https://i.imgur.com/8zNRwsn.png" width="400px" title="source: imgur.com" /></a>



'The'를 입력, 'Quick'을 정답으로 놓고 가중치 행렬을 업데이트한다. 그 다음엔 'The'를 입력, 'Brown'을 정답으로 놓는다. 이런 방식으로 말뭉치 전체를 훑으면 말뭉치의 분포 정보가 단어벡터에 내재하게 된다.

Word2Vec은 빠른 학습과 뛰어난 성능으로 오늘날 가장 주목받고 있는 벡터화 기술이다. 페이스북 연구진은 문서를 벡터화하는 [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)을, 스탠포드대학 연구진은 그래프(graph)를 벡터화하는 [Node2Vec](https://snap.stanford.edu/node2vec/)을 제안했다. 두 기술 모두 모체는 Word2Vec을 바탕으로 한다.

